### 第9章 数据管理

>本章主要讨论k8s如何管理存储资源

>学习目标

    A.Volume：k8s如何通过volume为集群中的容器提供存储
    
    B.实践常用volume类型及应用场景
    
    C.k8s如何通过Persistent Volume和Persistent Volume Claim分离集群管理员与集群用户职责
    
    D.实践Volume的静态供给和动态供给
    
#### 1.Volume--k8s存储模型

>目标：如何将各种持久化存储映射到容器

>问题

    容器和pod是短暂(生命周期短)的，会被频繁创建和销毁。容器销毁时，保存在容器内部文件系统中的数据都会被清除。
    
>上述为持久化存储容器的数据，可使用k8s volume

    Volume的生命周期独立于容器，Pod中的容器可能被销毁和重建，但Volume会被保留。
    
>本质上k8s Volume是一个目录，同Docker Volume

    Volume被mount到Pod,Pod中所有容器都可以访问这个Volume
    
>k8s Volume支持多种backend类型,完整列表参看[官方文档](https://kubernetes.io/docs/concepts/storage/volumes/)

    emptyDir,hostPath,GCE Persistent Disk,AWS Elastic Block Store,NFS,Ceph等
    
>Volume提供了对各种backend抽象

    容器在使用Volume读写数据时，不需关心数据存放在本地节点/云硬盘，对容器来说，所有类型Volume都只是一个目录
    
##### 1-1.emptyDir --最基础的Volume类型

>emptyDir Volume对容器来说是持久的，对于Pod则不是

    当Pod从节点删除，则Volume的内容也会被删除
    
    emptyDir Volume的生命周期与Pod一致
    
    emptyDir适合Pod中的容器需要临时共享存储空间的场景。如生产者，消费者用例
    
>实战

    ###Step1
    [root@node1 k8s]# cat emptyDir.yml 
    apiVersion: v1
    kind: Pod   ##创建pod资源
    metadata:
      name: producer-consumer
    spec:
      containers:
      - image: busybox
        name: producer
        volumeMounts:
        - mountPath: /producer_dir  ##2.producer容器将shared-volume mount到/producer_dir目录
          name: shared-volume
        args:
        - /bin/sh   ##3.通过echo将数据写入到文件hello里
        - -c
        - echo "hello k8s volume" > /producer_dir/hello; sleep 30000
      - image: busybox
        name: consumer
        volumeMounts:
        - mountPath: /consumer_dir ##4.consumer容器将shared-volume mount到/consumer_dir目录
          name: shared-volume
        args:
        - /bin/sh  ##5.通过cat从文件hello读数据
        - -c
        - cat /consumer_dir/hello; sleep 30000
      volumes:   
      - name: shared-volume
        emptyDir: {}   ##1.定义一个emptyDir类型的Volume shared-volume

    ###Step2.
    [root@node1 k8s]# kubectl apply -f emptyDir.yml 
    pod/producer-consumer created
    
    ###Step3.
    [root@node1 k8s]# kubectl get pod
    NAME                READY   STATUS    RESTARTS   AGE
    producer-consumer   2/2     Running   0          23m
    
    ###Step4.通过logs查看，显示consumer成功读到producer写入的数据,验证两个容器共享emptyDir Volume
    [root@node1 k8s]# kubectl logs producer-consumer consumer
    hello k8s volume
    
    
    说明：emptyDir是Docker Host文件系统里的目录。效果相当于执行：
    (1).docker run -v /producer_dir
    (2).docker run -v /consumer_dir
    
    
    ###进入pod，查看容器
    [root@node1 k8s]# kubectl --namespace=default exec -it producer-consumer --container producer -- sh
    / # ls
    bin           etc           proc          root          tmp           var
    dev           home          producer_dir  sys           usr
    
    [root@node1 k8s]# kubectl --namespace=default exec -it producer-consumer --container consumer -- sh
    / # ls
    bin           dev           home          root          tmp           var
    consumer_dir  etc           proc          sys           usr
    / # exit
    
##### 1-2. hostPath--持久性优于emptyDir

>hostPath作用：将Docker Host文件系统中已经存在的目录mount给pod容器。

    大部分应用不会使用hsotPath Volume,因为实际增加了Pod与节点的耦,限制了pod的使用
    
    需要访问k8s或docker内部数据(配置文件或二进制库)等应用则需要使用Hostpath
    
    如kube-apiserver和kube-controller-manager应用，查看kube-apiserver pod的配置:
    (1).$ kubectl get pod --namespace=kube-system
    (2).$kubectl edit --namespace=kube-system pod kube-apiserver-node1  ###查看kube-apiserver pod的配置
    ###截取部分
        volumeMounts: 
        - mountPath: /etc/ssl/certs  #对应host目录
          name: ca-certs   ##hostpath
          readOnly: true
        - mountPath: /etc/pki
          name: etc-pki
          readOnly: true
        - mountPath: /etc/kubernetes/pki
          name: k8s-certs
          readOnly: true
      dnsPolicy: ClusterFirst
      enableServiceLinks: true
      hostNetwork: true
      nodeName: node1
      priority: 2000000000
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      tolerations:
      - effect: NoExecute
        operator: Exists
      volumes:
      - hostPath:
          path: /etc/ssl/certs
          type: DirectoryOrCreate
        name: ca-certs
      - hostPath:
          path: /etc/pki
          type: DirectoryOrCreate
        name: etc-pki
      - hostPath:
          path: /etc/kubernetes/pki
          type: DirectoryOrCreate
        name: k8s-certs
        
>Pod销毁，hostPath对应的目录还会被保留，hostPath持久性优于emptyDir，但Host崩溃，则hostPath也就无法访问

##### 1-3. 外部storage provider --真正持久性Volume

    k8s若部署在AWS,GCE,AZURE等公有云上，则可直接使用云硬盘作为Volume
    
>实践:AWS Elastic Block Store

    ###Step1.
    [root@node1 k8s]# cat aws.yml
    apiVersion: v1
    kind: Pod
    metadata:
      name: using-ebs
    spec:
      containers:
      - image: busybox
        name: using-ebs
        volumeMounts:
        - mountPath: /test-ebs
          name: ebs-volume
      volumes:
      - name: ebs-volume
      #this AWS EBS volume must already exist
        awsElasticBlockStore:
          volumeID: <volume-id>
          fsType: ext4
          
    说明：要在pod中使用ESB Volume，必须先在AWS中创建，然后通过volume-id引用
    
> k8s volume也可使用注流分布式存储，如Ceph,GlusterFS等

    ###Ceph例子
    [root@node1 k8s]# cat ceph.yml 
    apiVersion: v1
    kind: Pod
    metadata:
      name: using-ceph
    spec:
      containers:
        - image: busybox
          name: using-ceph
          volumeMounts:
          - name: ceph-volume
            mountPath: /test-ceph
      volumes:
        - name: ceph-volume
          cephfs:
            path: /some/path/in/side/cephfs  ##1.ceph文件系统的该目录被mount到容器路径/test-ceph
            monitors: "10.16.154.78:6789"
            secretFile: "/etc/ceph/admin.secret"
    
    说明：
    
    相对于emptyDir和hostPath，这些Volume最大特点不依赖k8s
    
    Volume的底层基础设施由独立的存储系统管理，与k8s集群是分离的。
    
    数据被持久化后，即使整个k8s崩溃也不会受损
    
#### 2.PersistentVolume(PV) & PersistentVolumeClaim(PVC)

参考:[k8s持久化存储PV & UV](https://www.cnblogs.com/benjamin77/p/9944268.html)

>Volume提供了好的数据持久化方案，但管理性上还有不足
    
    如前AWS EBS例：要使用Volume,Pod必须先知道
    A.当前Volume来自AWS EBS
    B.EBS Volume已提前创建且知道确切volume-id
    
    pod通常由应用开发人员维护，Volume则由存储系统管理员维护。（产生问题-耦合）(解决方案-k8s PV和PVC )
    
 
>PersistentVolume(PV):是外部存储系统中的一块存储空间(由管理员创建/维护)
 
    与Volume一样，PV具有持久性，生命周期独立于Pod.
    
>PersistentVolumeClaim(PVC):是对PV的申请(Claim)(通常由普通用户创建和维护)
 
    需要为Pod分配存储资源时，用户可创建一个PVC,指明存储资源的容量大小和访问模式(如只读)等信息。k8s会查找并提供满足条件的PV
    
>说明

    有了PVC,用户只需告诉k8s需要什么样的存储资源，而不必关心真正的空间从哪分配，如何访问等底层细节信息。
    
    这些storage provider底层信息交由管理员处理
    
>k8s支持多类型的PersistentVolume(PV),[参见官方文档](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)

    AWS EBS,Ceph,NFS
    
##### 2-1.NFS PersistentVolume

>Step1.node1(master)节点搭建NFS服务器

参见[NFS服务器](../NFS服务器.md)

    ###1。安装nfs
    $ yum -y install nfs-utils rpcbind
    
    ###2。在NFS服务端上创建共享目录/export/nfs并设置权限
    $ mkdir -p /nfsdata
    $ chmod 666 /nfsdata
    
    ###3。编辑export文件
    $ vim /etc/exports
    
    ###add contents,将/nfsdata这个目录共享给192.168.1.*这些客户机
    [root@node1 ~]# cat /etc/exports
    /nfsdata 192.168.1.0/24(rw,no_root_squash,no_all_squash,sync)
    或
    /nfsdata *(rw,no_root_squash,no_all_squash,sync)   ##*代表所有机器
    
    ###4。配置生效
    $ exportfs -r
    
    ###5。启动rpcbind、nfs服务
    $ service rpcbind start
    $ service nfs start
    
    ###6。查看 RPC 服务的注册状况
    $ rpcinfo -p localhost  //不加localhost默认为本机
    
    ###7。server 端先自我测试一下是否可以联机
    $ showmount -e localhost //不加localhost默认为本机
    [root@node1 ~]# showmount -e
    Export list for node1:
    /nfsdata 192.168.1.0/24    ##代表允许192.168.1.*的所有客户机挂载到目录/export/nfs目录下
    
    或 
    [root@node1 nfsdata]# showmount -e
    Export list for node1:
    /nfsdata *  #用*允许所有主机
    
>Step2.创建一个PV mypv1,配置文件nfs-pv1.yml(准备PV资源)

    ###1.
    [root@node1 k8s]# cat nfs-pv1.yml 
    apiVersion: v1
    kind: PersistentVolume  ##资源类型为持久化存储
    metadata:
      name: mypv1
    spec:
      capacity: ##1.指定PV的容器为1GB
        storage: 1Gi
      accessModes: ##2.指定访问模式为ReadWriteOnce(支持三种访问模式:ReadWriteOnce/ReadOnlyMany/ReadWriteMany)
        - ReadWriteOnce
      persistentVolumeReclaimPolicy: Recycle  ##3.指定当PV的回收策略为Recycle,支持三种回收策略(Recycle/Retain/Delete)
      storageClassName: nfs  ##4.指定PV的class为nfs。相当于为PV设置了一个分类，PVC可以指定class申请相应的class的PV
      nfs:
        path: /nfsdata/pv1  ##5.指定PV在NFS服务器上对应的目录(注意：事先需要先在/nfsdata目录下创建目录pv1)
        server: 192.168.1.31
        
    ###2.创建mypv1
    [root@node1 k8s]# kubectl apply -f nfs-pv1.yml 
    persistentvolume/mypv1 created
    
    ###3.查看pv
    [root@node1 k8s]# kubectl get pv
    NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
    mypv1   1Gi        RWO            Recycle          Available           nfs                     8s

    上述：status为Available表mypv1已就绪，可以被PVC申请
    
    accessModes三种访问模式：
    (1).ReadWriteOnce:表示PV能以read-write模式mount到单个节点
    (2).ReadOnlyMany:表示PV能以read-only模式mount到多个节点
    (3).ReadWriteMany:表示PV能以read-write模式mount到多个节点
    
    persistentVolumeReclaimPolicy三种策略
    (1).Retain:表需要管理员手工回收
    (2).Recycle:表清除PV中的数据,效果相当于rm -rf /thevolume/*
    (3).Delete:表删除Storage Provider上的对应存储资源。如AWS EBS,GCE PD,Azure Disk,OpenStack Cinder Volume等
    
>Step3.创建 PVC mypvc1,配置文件nfs-pvc1.yml(申请PV资源)

    ###1.
    [root@node1 k8s]# cat nfs-pvc1.yml 
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: mypvc1
    spec:
      accessModes:  ##指定访问模式
        - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi ##指定PV的容量
      storageClassName: nfs ##指定PV的class为nfs

    ###2.创建mypvc1
    [root@node1 k8s]# kubectl apply -f nfs-pvc1.yml 
    persistentvolumeclaim/mypvc1 created
    
    ###3.查询pvc,发现mypvc1已经绑定到mypv1，申请成功
    [root@node1 k8s]# kubectl get pvc
    NAME     STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
    mypvc1   Bound    mypv1    1Gi        RWO            nfs            5s
    
    ###4.查看pv,此时CLAIM有值
    [root@node1 k8s]# kubectl get pv
    NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM            STORAGECLASS   REASON   AGE
    mypv1   1Gi        RWO            Recycle          Bound    default/mypvc1   nfs                     40m

>Step4.Pod中使用存储

    ###1.编写pod1.yml
    [root@node1 k8s]# cat pod1.yml 
    apiVersion: v1
    kind: Pod
    metadata:
      name: mypod1
    spec:
      containers:
        - name: mypod1
          image: busybox
          args:
          - /bin/sh
          - -c
          - sleep 30000
          volumeMounts:
          - mountPath: "/mydata"
            name: mydata
      volumes:
        - name: mydata
          persistentVolumeClaim:
            claimName: mypvc1
            
    ###2.创建pod资源
    [root@node1 k8s]# kubectl apply -f pod1.yml 
    pod/mypod1 created
    
    ###3.若上述创建有问题，可能过下述命令查看出错原因
    $ kubectl describe pod mypod
    
    ###4.查看pod状态信息
    [root@node1 k8s]# kubectl get pod -o wide
     NAME     READY   STATUS    RESTARTS   AGE   IP              NODE    NOMINATED NODE   READINESS GATES
     mypod1   1/1     Running   0          11s   10.244.135.15   node3   <none>           <none>
    
    ###5.验证PV是否可用
    [root@node1 k8s]# kubectl exec mypod1 touch /mydata/hello-pvc
    [root@node1 k8s]# ls /nfsdata/pv1/
    hello-pvc
    
    上述：在Pod中创建的文件/mydata/hello-pvc已保存到NFS服务器目录/nfsdata/pv1中
    
    ###6.进入pod查看容器
    [root@node1 k8s]# kubectl exec -it mypod1 -- sh
    / # ls
    bin     dev     etc     home    mydata  proc    root    sys     tmp     usr     var
    / # ls mydata
    hello-pvc
    
    ###7.删除pod
    [root@node1 k8s]# kubectl delete pod mypod1
    pod "mypod1" deleted
    
##### 2-2.回收PV

>当不需要使用PV时，可用删除PVC回收PV

    ###1.删除pvc(删除pvc前需要先删除pod,否则删除pvc无法执行)
    [root@node1 k8s]# kubectl delete pvc mypvc1
    persistentvolumeclaim "mypvc1" deleted
    
    ###2.查看pod,k8s启动了一个新的pod,作用是清除PV mypv1的数据
    [root@node1 k8s]# kubectl get pod -o wide
    NAME                 READY   STATUS              RESTARTS   AGE   IP       NODE    NOMINATED NODE   READINESS GATES
    recycler-for-mypv1   0/1     ContainerCreating   0          8s    <none>   node3   <none>           <none>
    
    ###3.查看mypv1状态，为Released表示已经解除了与mypvc1的Bound，status="Released"表示正在清除数据，此时还不可用,当status='Available'时代表该PV可被新的PVC申请
    [root@node1 k8s]# kubectl get pv
    NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
    mypv1   1Gi        RWO            Recycle          Available           nfs                     14h
    
    ###4.查看pvc已被清除
    [root@node1 k8s]# kubectl get pvc
    No resources found.  
    
    ###5.数据清除完毕，mypv1的状态重新变为Available，此时可被新的PVC申请 
    
    ###6.查看NFS服务器上的文件hello-pvc已被删除了
    [root@node1 k8s]# ls /nfsdata/pv1/
    
    ###7.删除pv
    [root@node1 k8s]# kubectl delete pv mypv1
    persistentvolume "mypv1" deleted
    
>因PV的回收策略设置为Recycle，所以数据会被清除。若需保留数据，则可以将策略设置为Retain

>实战：回收策略设置为Retain

    ###1.
    [root@node1 k8s]# cat nfs-pv2.yml 
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: mypv2
    spec:
      capacity:
        storage: 1Gi
      accessModes:
        - ReadWriteOnce
      persistentVolumeReclaimPolicy: Retain
      storageClassName: nfs
      nfs:
        path: /nfsdata/pv2   ##需要在NFS服务器/nfsdata目录下创建pv2目录
        server: 192.168.1.31
        
    ###2.创建pv,回收类型为Retain
    [root@node1 k8s]# kubectl apply -f nfs-pv2.yml 
    persistentvolume/mypv2 created
    [root@node1 k8s]# kubectl get pv
    NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
    mypv2   1Gi        RWO            Retain           Available           nfs                     4s

    ###3.
    [root@node1 k8s]# cat nfs-pvc2.yml 
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: mypvc2
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
      storageClassName: nfs
      
    ###4.创建pvc
    [root@node1 k8s]# kubectl apply -f nfs-pvc2.yml 
    persistentvolumeclaim/mypvc2 created
    [root@node1 k8s]# kubectl get pvc
    NAME     STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
    mypvc2   Bound    mypv2    1Gi        RWO            nfs            3s
    
    ###5.
    [root@node1 k8s]# cat pod2.yml 
    apiVersion: v1
    kind: Pod
    metadata:
      name: mypod2
    spec:
      containers:
        - name: mypod2
          image: busybox
          args:
          - /bin/sh
          - -c
          - sleep 30000
          volumeMounts:
          - mountPath: "/mydata"
            name: mydata
      volumes:
        - name: mydata
          persistentVolumeClaim:
            claimName: mypvc2

    ###6.创建pod资源
    [root@node1 k8s]# kubectl apply -f pod2.yml 
    pod/mypod2 created
    
    ###7.查看pod状态
    [root@node1 k8s]# kubectl get pod -o wide
    NAME     READY   STATUS    RESTARTS   AGE   IP              NODE    NOMINATED NODE   READINESS GATES
    mypod2   1/1     Running   0          11s   10.244.104.45   node2   <none>           <none>
    
    ###8.测试，在Pod中创建文件
    [root@node1 k8s]# kubectl exec mypod2 touch /mydata/hello-pv
    
    ###9.查看NFS服务器该文件已存在
    [root@node1 k8s]# ls /nfsdata/pv2/
    hello-pv
    
    ###10.回收pvc
    [root@node1 k8s]# kubectl delete pod mypod2
    pod "mypod2" deleted
    
    [root@node1 k8s]# kubectl delete pvc mypvc2
    persistentvolumeclaim "mypvc2" deleted
    
    [root@node1 k8s]# kubectl get pv
    NAME    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM            STORAGECLASS   REASON   AGE
    mypv2   1Gi        RWO            Retain           Released   default/mypvc2   nfs                     9m37s
    
    ###此时再查看NFS服务器目录,文件依然存在
    [root@node1 k8s]# ls /nfsdata/pv2/
    hello-pv
    
    **上述：虽然mypv2中的数据得到了保留，但其PV状态一直处于Released,不能被其他PVC申请.为了重新使用资源，可删除并重新创建mypv1。删除操作只是删除了PV对象，存储空间中的数据并不会被删除**
    
    [root@node1 k8s]# kubectl delete pv mypv2
    persistentvolume "mypv2" deleted
    
    **另：PV还支持Delete回收策略，会删除PV在Storage Provider上对应的存储空间。NFS的PV不支持Delete.支持Delete的Provier有AWS EBS,GCE PD,Azure Disk,Openstack Cinder Volume等**
    
##### 2-3.PV动态供给--StorageClass

    上述例子，提前创建PV，通过PVC申请PV并在Pod中使用  --此种方式叫静态供给(static provision)
    
    与静态供给相对应--动态供给(Dynamical Provision):即如果没有满足PVC条件的PV，会动态创建PV
    
    动态供给较静态供给优势：不需要提前创建PV，减少了管理工作量，效率高
    
    动态供给是通过StorageClass实现的，StorageClass定义了如何创建PV。StorageClass支持Delete和Retain两种reclaimPolicy，默认Delete
    
>实战1。StorageClass standard -动态创建PV

    [root@node1 k8s]# cat StorageClass-standard.yml 
    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: standard  ##standard创建的是gp2类型的EBS
    provisioner: kubernetes.io/aws-ebs
    parameters:
      type: gp2
    reclaimPolicy: Retain
    

>实战2。StorageClass slow -动态创建PV

    [root@node1 k8s]# cat StorageClass-slow.yml 
    apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
      name: slow ##slow创建的是io1类型的EBS
    provisioner: kubernetes.io/aws-ebs
    parameters:
      type: io1
      zones: us-east-1d, us-east-1c
      iopsPerGB: "19"
      
    StorageClass支持Delete和Retain两种reclaimPolicy回收策略,默认是Delete
    
>上述，PVC在申请PV时，只需指定StorageClass，容量及访问模式即可

    [root@node1 k8s]# cat nfs-pvc3.yml 
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: mypvc3
    spec:
      accessModes:
        - ReadWriteOnce ##指定访问模式
      resources:
        requests:
          storage: 1Gi ##指定容量
      storageClassName: standard ##指定StorageClass
    
除AWS EBS,k8s还支持多种动态供给PV的Provisioner。[参见官方文档](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner)

#### 3.一个数据库例子

>目标:如何为Mysql数据库提供持久化存储

>步骤:

    (1).创建PV和PVC
    (2).部署mysql
    (3).向mysql添加数据
    (4).模拟节点宕机故障，k8s将mysql自动迁移到其他节点
    (5).验证数据一致性
    
> 实战:

    ###准备
    [root@node1 mysql]# mkdir -p /nfsdata/mysql-pv

1.创建PV和PVC

    ###1.yaml配置
    [root@node1 mysql]# cat mysql-pv.yml 
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: mysql-pv
    spec:
      accessModes:
        - ReadWriteOnce
      capacity:
        storage: 1Gi
      persistentVolumeReclaimPolicy: Retain
      storageClassName: nfs
      nfs:
        path: /nfsdata/mysql-pv
        server: 192.168.1.31
    
    ###2. 
    [root@node1 mysql]# cat mysql-pvc.yml 
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata: 
      name: mysql-pvc
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
      storageClassName: nfs
      
    ###3.创建pv
    [root@node1 mysql]# kubectl apply -f mysql-pv.yml 
    persistentvolume/mysql-pv created
    
    ###4.pvc申请
    [root@node1 mysql]# kubectl apply -f mysql-pvc.yml 
    persistentvolumeclaim/mysql-pvc created
    
    ###查询
    [root@node1 mysql]# kubectl get pv,pvc
    NAME                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS   REASON   AGE
    persistentvolume/mysql-pv   1Gi        RWO            Retain           Bound    default/mysql-pvc   nfs                     43s
    
    NAME                              STATUS   VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
    persistentvolumeclaim/mysql-pvc   Bound    mysql-pv   1Gi        RWO            nfs            30s
      
2.部署mysql

    ###1.mysql.yml(此处5.6镜像。5.7版本镜像拉取会报错)
    [root@node1 mysql]# cat mysql.yml 
    apiVersion: v1
    kind: Service
    metadata:
      name: mysql
    spec:
      ports:
      - port: 3306
      selector:
        app: mysql
    
    ---
    apiVersion: apps/v1beta1
    kind: Deployment
    metadata:
      name: mysql
    spec:
      selector:
        matchLabels:
          app: mysql
      template:
        metadata:
          labels:
            app: mysql
        spec:
          containers:
          - image: mysql:5.6
            name: mysql
            env:
            - name: MYSQL_ROOT_PASSWORD
              value: password
            ports:
            - containerPort: 3306
              name: mysql
            volumeMounts:
            - name: mysql-persistent-storage
              mountPath: /var/lib/mysql
          volumes:
          - name: mysql-persistent-storage
            persistentVolumeClaim:
              claimName: mysql-pvc

    ###2.创建资源
    [root@node1 mysql]# kubectl apply -f mysql.yml 
    service/mysql created
    deployment.apps/mysql created
    
    ###3.查看deployment
    [root@node1 mysql]# kubectl get deployment -o wide
    NAME    READY   UP-TO-DATE   AVAILABLE   AGE     CONTAINERS   IMAGES      SELECTOR
    mysql   1/1     1            1           2m43s   mysql        mysql:5.6   app=mysql
    
    ###4.查看pod(mysql被部署到node2节点上)
    [root@node1 mysql]# kubectl get pod -o wide
    NAME                     READY   STATUS    RESTARTS   AGE     IP              NODE    NOMINATED NODE   READINESS GATES
    mysql-7686899cf9-tggfs   1/1     Running   0          2m58s   10.244.104.47   node2   <none>           <none>
    
    ###5.查看service
    [root@node1 mysql]# kubectl get service
    NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
    kubernetes   ClusterIP   10.1.0.1       <none>        443/TCP    46d
    mysql        ClusterIP   10.1.116.144   <none>        3306/TCP   3m18s
    
    ##6.查看详细日志
    $ kubectl describe pod mysql-7686899cf9-tggfs
    $ kubectl describe deployment mysql
    
    ###7.客户端访问Service mysql
    [root@node1 mysql]# kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql -ppassword
    If you don't see a command prompt, try pressing enter.
    Welcome to the MySQL monitor.  Commands end with ; or \g.
    Your MySQL connection id is 1
    Server version: 5.6.44 MySQL Community Server (GPL)
    
    Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
    
    Oracle is a registered trademark of Oracle Corporation and/or its
    affiliates. Other names may be trademarks of their respective
    owners.
    
    Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
    
    mysql> create database syf;
    Query OK, 1 row affected (0.01 sec)
    
    mysql> use syf
    Database changed
    mysql> create table my_id(id int(4));
    Query OK, 0 rows affected (0.05 sec)
    
    mysql> insert into my_id values(111);
    Query OK, 1 row affected (0.00 sec)
    
    mysql> select * from my_id;
    +------+
    | id   |
    +------+
    |  111 |
    +------+
    1 row in set (0.00 sec)
    
    ###8.模拟node2节点宕机故障
    [root@node2 ~]# shutdown now
    Connection to node2 closed by remote host.
    Connection to node2 closed.
    
    ###9.查看pod,k8s将mysql迁移到node3节点上(需要一段时间)
    [root@node1 k8s]# kubectl get pod -o wide
    NAME                     READY   STATUS        RESTARTS   AGE   IP              NODE    NOMINATED NODE   READINESS GATES
    mysql-7686899cf9-rkg7q   1/1     Running       0          9s    10.244.135.20   node3   <none>           <none>
    mysql-7686899cf9-tggfs   1/1     Terminating   0          18m   10.244.104.47   node2   <none>           <none>
    mysql-client             1/1     Running       0          10m   10.244.135.19   node3   <none>           <none>
    
    ###10.验证数据一致性(mysql服务恢复，数据也完好无损)
    [root@node1 k8s]# kubectl exec -it mysql-client -- mysql -h mysql -ppassword
    Warning: Using a password on the command line interface can be insecure.
    Welcome to the MySQL monitor.  Commands end with ; or \g.
    Your MySQL connection id is 1
    Server version: 5.6.44 MySQL Community Server (GPL)
    
    Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.
    
    Oracle is a registered trademark of Oracle Corporation and/or its
    affiliates. Other names may be trademarks of their respective
    owners.
    
    Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.
    
    mysql> use syf
    Reading table information for completion of table and column names
    You can turn off this feature to get a quicker startup with -A
    
    Database changed
    mysql> select * from my_id;
    +------+
    | id   |
    +------+
    |  111 |
    +------+
    1 row in set (0.00 sec)
    
>小结

    1.emptyDir和hostPath类型的Volume很方便，但持久性不强，k8s支持多种外部存储系统的volume
    
    2.pv和pvc分离了管理员和普通用户的职责，更适合生产环境。并学习如何通过StorageClass实现更高效的动态供给
    
    3.如何在mysql中使用PersistentVolume实现数据持久化性
    
    
--------------------------------------------

**体胖还需勤跑步，人丑就要多读书!!! --开心玉凤**