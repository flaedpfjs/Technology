### 第5章 运行应用

    k8s通过各种Controller来管理Pod的生命周期。为满足不同业务场景，k8s开发了Deployment,ReplicaSet,DaemonSet,StatefuleSet,Job等多种Controller。(kind值指定)
    
#### 1。Deployment

##### 1-1.运行Deployment

     ###创建2个副本nginx-deployment  --image指定镜像
     [root@node1 ~]# kubectl run nginx-deployment --image=nginx:1.7.9 --replicas=2
     kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
     deployment.apps/nginx-deployment created
     
     ------------------------
     
     ###查看nginx-deployment的状态，输出显示两个副本正常运行
     [root@node1 ~]# kubectl get deployment nginx-deployment
     NAME               READY   UP-TO-DATE   AVAILABLE   AGE
     nginx-deployment   2/2     2            2           2m33s
     
     
     ###查看deployment详细信息
     [root@node1 ~]# kubectl describe deployment
     Name:                   nginx-deployment
     Namespace:              default
     CreationTimestamp:      Fri, 10 May 2019 01:44:16 +0000
     Labels:                 run=nginx-deployment
     Annotations:            deployment.kubernetes.io/revision: 1
     Selector:               run=nginx-deployment
     Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable
     StrategyType:           RollingUpdate
     MinReadySeconds:        0
     RollingUpdateStrategy:  25% max unavailable, 25% max surge
     Pod Template:
       Labels:  run=nginx-deployment
       Containers:
        nginx-deployment:
         Image:        nginx:1.7.9
         Port:         <none>
         Host Port:    <none>
         Environment:  <none>
         Mounts:       <none>
       Volumes:        <none>
     Conditions:
       Type           Status  Reason
       ----           ------  ------
       Available      True    MinimumReplicasAvailable
       Progressing    True    NewReplicaSetAvailable
     OldReplicaSets:  <none>
     NewReplicaSet:   nginx-deployment-578fb949d8 (2/2 replicas created)
     Events: ###Events是Deployment的日志,记录了ReplicaSet启动过程,同时验证了Deployment通过ReplicaSet来管理Pod
       Type    Reason             Age    From                   Message
       ----    ------             ----   ----                   -------
       Normal  ScalingReplicaSet  3m10s  deployment-controller  Scaled up replica set nginx-deployment-578fb949d8 to 2
       
     ------------------------
     
      ###查看两个副本已就绪,Name即为上述Events中的Message(Scaled up replica set "nginx-deployment-578fb949d8" to 2)
      [root@node1 ~]# kubectl get replicaset
       NAME                          DESIRED   CURRENT   READY   AGE
       nginx-deployment-578fb949d8   2         2         2       15m
       
       
      ###查看详细信息
      [root@node1 ~]# kubectl describe replicaset
      Name:           nginx-deployment-578fb949d8
      Namespace:      default
      Selector:       pod-template-hash=578fb949d8,run=nginx-deployment
      Labels:         pod-template-hash=578fb949d8
                      run=nginx-deployment
      Annotations:    deployment.kubernetes.io/desired-replicas: 2
                      deployment.kubernetes.io/max-replicas: 3
                      deployment.kubernetes.io/revision: 1
      Controlled By:  Deployment/nginx-deployment  ###指明此ReplicaSet是由Deployment nginx-deployment创建的
      Replicas:       2 current / 2 desired   ###Deployment创建Replicas
      Pods Status:    2 Running / 0 Waiting / 0 Succeeded / 0 Failed
      Pod Template:
        Labels:  pod-template-hash=578fb949d8
                 run=nginx-deployment
        Containers:
         nginx-deployment:
          Image:        nginx:1.7.9
          Port:         <none>
          Host Port:    <none>
          Environment:  <none>
          Mounts:       <none>
        Volumes:        <none>
      Events: ###两个副本日志
        Type    Reason            Age   From                   Message
        ----    ------            ----  ----                   -------
        Normal  SuccessfulCreate  14m   replicaset-controller  Created pod: nginx-deployment-578fb949d8-4h4sg
        Normal  SuccessfulCreate  14m   replicaset-controller  Created pod: nginx-deployment-578fb949d8-6bfjf
        
      ###上述Events->Message创建两个副本,对象命名: 名称=父对象名称+随机字符串或数字
        
     ------------------------
     
     ###查看pod
     [root@node1 ~]# kubectl get pod
     NAME                                READY   STATUS    RESTARTS   AGE
     nginx-deployment-578fb949d8-4h4sg   1/1     Running   0          23m
     nginx-deployment-578fb949d8-6bfjf   1/1     Running   0          23m
     
     
     ###查看更详细信息
     [root@node1 ~]# kubectl describe pod
     Name:               nginx-deployment-578fb949d8-4h4sg
     Namespace:          default
     Priority:           0
     PriorityClassName:  <none>
     Node:               node2/192.168.1.32
     Start Time:         Fri, 10 May 2019 01:44:16 +0000
     Labels:             pod-template-hash=578fb949d8
                         run=nginx-deployment
     Annotations:        cni.projectcalico.org/podIP: 10.244.104.6/32
     Status:             Running
     IP:                 10.244.104.6
     Controlled By:      ReplicaSet/nginx-deployment-578fb949d8 ###指明此Pod是由ReplicaSet/nginx-deployment-578fb949d8创建的
     Containers:
       nginx-deployment:
         Container ID:   docker://78b084d9b37b0c7b41851d2564c3265d0ea9e4d20abad03c1587d8e71c855a66
         Image:          nginx:1.7.9
         Image ID:       docker-pullable://nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451
         Port:           <none>
         Host Port:      <none>
         State:          Running
           Started:      Fri, 10 May 2019 01:46:48 +0000
         Ready:          True
         Restart Count:  0
         Environment:    <none>
         Mounts:
           /var/run/secrets/kubernetes.io/serviceaccount from default-token-gtn9w (ro)
     Conditions:
       Type              Status
       Initialized       True 
       Ready             True 
       ContainersReady   True 
       PodScheduled      True 
     Volumes:
       default-token-gtn9w:
         Type:        Secret (a volume populated by a Secret)
         SecretName:  default-token-gtn9w
         Optional:    false
     QoS Class:       BestEffort
     Node-Selectors:  <none>
     Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                      node.kubernetes.io/unreachable:NoExecute for 300s
     Events:
       Type     Reason          Age                From               Message
       ----     ------          ----               ----               -------
       Normal   Scheduled       24m                default-scheduler  Successfully assigned default/nginx-deployment-578fb949d8-4h4sg to node2
       Warning  Failed          23m                kubelet, node2     Failed to pull image "nginx:1.7.9": rpc error: code = Unknown desc = context canceled
       Warning  Failed          23m                kubelet, node2     Error: ErrImagePull
       Normal   SandboxChanged  23m                kubelet, node2     Pod sandbox changed, it will be killed and re-created.
       Normal   BackOff         23m (x3 over 23m)  kubelet, node2     Back-off pulling image "nginx:1.7.9"
       Warning  Failed          23m (x3 over 23m)  kubelet, node2     Error: ImagePullBackOff
       Normal   Pulling         22m (x2 over 24m)  kubelet, node2     pulling image "nginx:1.7.9"
       Normal   Pulled          22m                kubelet, node2     Successfully pulled image "nginx:1.7.9"
       Normal   Created         22m                kubelet, node2     Created container
       Normal   Started         22m                kubelet, node2     Started container
     
     
     Name:               nginx-deployment-578fb949d8-6bfjf
     Namespace:          default
     Priority:           0
     PriorityClassName:  <none>
     Node:               node3/192.168.1.33
     Start Time:         Fri, 10 May 2019 01:44:16 +0000
     Labels:             pod-template-hash=578fb949d8
                         run=nginx-deployment
     Annotations:        cni.projectcalico.org/podIP: 10.244.135.1/32
     Status:             Running
     IP:                 10.244.135.1
     Controlled By:      ReplicaSet/nginx-deployment-578fb949d8
     Containers:
       nginx-deployment:
         Container ID:   docker://cf0aa2d9467a653879a2e4f795463e454f7e9a676d018e44a539ac3e985fb46a
         Image:          nginx:1.7.9
         Image ID:       docker-pullable://nginx@sha256:e3456c851a152494c3e4ff5fcc26f240206abac0c9d794affb40e0714846c451
         Port:           <none>
         Host Port:      <none>
         State:          Running
           Started:      Fri, 10 May 2019 01:45:17 +0000
         Ready:          True
         Restart Count:  0
         Environment:    <none>
         Mounts:
           /var/run/secrets/kubernetes.io/serviceaccount from default-token-gtn9w (ro)
     Conditions:
       Type              Status
       Initialized       True 
       Ready             True 
       ContainersReady   True 
       PodScheduled      True 
     Volumes:
       default-token-gtn9w:
         Type:        Secret (a volume populated by a Secret)
         SecretName:  default-token-gtn9w
         Optional:    false
     QoS Class:       BestEffort
     Node-Selectors:  <none>
     Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                      node.kubernetes.io/unreachable:NoExecute for 300s
     Events: ###日志,记录Pod启动过程
       Type    Reason     Age   From               Message
       ----    ------     ----  ----               -------
       Normal  Scheduled  24m   default-scheduler  Successfully assigned default/nginx-deployment-578fb949d8-6bfjf to node3
       Normal  Pulling    24m   kubelet, node3     pulling image "nginx:1.7.9"
       Normal  Pulled     23m   kubelet, node3     Successfully pulled image "nginx:1.7.9"
       Normal  Created    23m   kubelet, node3     Created container
       Normal  Started    23m   kubelet, node3     Started container
       
      ------------------------
      
      ###上述创建nginx-deployment默认在default命名空间下
      [root@node1 nginx]# kubectl get deployments -n default
      NAME               READY   UP-TO-DATE   AVAILABLE   AGE
      nginx-deployment   2/2     2            2           44m
       
      ------------------------
      
      ###总结：
      
      A.用户通过kubectl创建Deployment
      B.Deployment创建ReplicaSet
      C.ReplicaSet创建Pod
     
      kubectl->Deployment->ReplicaSet->Pod
      

##### 1-2.命令 VS 配置文件

k8s支持两种创建资源方式：

1.kubectl命令直接创建

简单，适合临时测试和实验
  
    ###命令行中通过参数指定资源的属性
    $ kubectl run nginx-deployment --image=nginx:1.7.9 --replicas=2
    
    ###删除命令创建的deployment
    [root@node1 nginx]# kubectl delete deployment/nginx-deployment
    deployment.extensions "nginx-deployment" deleted
    
    ###查看应用
    [root@node1 nginx]# kubectl get deployment
    No resources found.

2.通过配置文件/kubectl apply创建

配置文件提供了创建资源模版，能够重复部署，方便管理，适合正式，跨环境，规模化部署

    Step1:创建nginx.yaml,资源的属性写在配置文件中，文件格式为YAML
    [root@node1 nginx]# cat nginx.yaml 
    apiVersion: extensions/v1beta1   ###当前配置格式的版本
    kind: Deployment    ###要创建的资源类型
    metadata: ###该资源的元数据
      name: nginx-deployment     ###必需的元数据项
      namespace: kube-public     ###指定命名空间，不指定会应用默认default命名空间
    spec:     ###该deployment的规格说明
      replicas: 2    ###指明副本数量，默认为1
      template:      ###定义pod的模版，此处为配置文件重要部分
        metadata:    ###定义pod的元数据，至少定义一个label
          labels:
            app: web_server ###label的key和value任意指定
        spec:        ###描述pod规格,此部分定义pod中每一个容器的属性
          containers:
          - name: nginx  ###必需
            image: nginx:1.7.9   ###必需
            
    ----------------------------------------------------------
    
    Step2:$ kubectl apply -f nginx.yaml  ###配置文件方式启动应用
    
    [root@node1 nginx]# kubectl apply -f nginx.yaml 
    deployment.extensions/nginx-deployment created
    
    [root@node1 nginx]# kubectl get deployment -n kube-public
    NAME               READY   UP-TO-DATE   AVAILABLE   AGE
    nginx-deployment   2/2     2            2           76m
    
    [root@node1 nginx]# kubectl get replicaset -n kube-public
    NAME                          DESIRED   CURRENT   READY   AGE
    nginx-deployment-65998d8886   2         2         2       76m
    
    [root@node1 nginx]# kubectl get pod -o wide -n kube-public
    NAME                                READY   STATUS    RESTARTS   AGE   IP             NODE    NOMINATED NODE   READINESS GATES
    nginx-deployment-65998d8886-hb7rk   1/1     Running   0          77m   10.244.135.2   node3   <none>           <none>
    nginx-deployment-65998d8886-zfvc8   1/1     Running   0          77m   10.244.104.7   node2   <none>           <none>

    ###以上，Deployment,ReplicaSet,Pod均已就绪
    
**kubectl apply:创建，更新资源**

    kubectl apply不但能创建k8s资源，也可对k8s资源更新
    
    k8s也提供了几个类似命令：kubectl create / kubectl replace / kubectl edit / kubectl patch  
    
    以上，尽量应用kubectl apply
    
**kubectl delete:删除资源**

    [root@node1 nginx]# kubectl delete -f nginx.yaml 
    deployment.extensions "nginx-deployment" deleted

    [root@node1 nginx]# kubectl get deployment -n kube-public
    No resources found.

##### 1-3.伸缩:在线增加/减少Pod副本数

    ### Step0.修改nginx.yaml，应用默认namespace且副本数量修改为5(2个副本时，在node2和node3上各跑一个副本)
    [root@node1 nginx]# cat nginx.yaml 
    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
      name: nginx-deployment
    spec:
      replicas: 5
      template:
        metadata:
          labels:
            app: web_server
        spec:
          containers:
          - name: nginx
            image: nginx:1.7.9
            
    ### Step1.创建/更新资源
    [root@node1 nginx]# kubectl apply -f nginx.yaml 
    deployment.extensions/nginx-deployment created
    
    ### Step2.查看pod,增加3个副本被调度到node2和node3
    [root@node1 nginx]# kubectl get pod -o wide 
    NAME                                READY   STATUS    RESTARTS   AGE   IP              NODE    NOMINATED NODE   READINESS GATES
    nginx-deployment-65998d8886-c7r4j   1/1     Running   0          53s   10.244.135.3    node3   <none>           <none>
    nginx-deployment-65998d8886-lp45t   1/1     Running   0          53s   10.244.104.10   node2   <none>           <none>
    nginx-deployment-65998d8886-nq7k2   1/1     Running   0          53s   10.244.135.4    node3   <none>           <none>
    nginx-deployment-65998d8886-wcs9r   1/1     Running   0          53s   10.244.104.8    node2   <none>           <none>
    nginx-deployment-65998d8886-xk6wk   1/1     Running   0          53s   10.244.104.9    node2   <none>           <none>

    ### Step3.减少副本数量为3，再更新资源，查看pod
    [root@node1 nginx]# kubectl apply -f nginx.yaml 
    deployment.extensions/nginx-deployment configured

    [root@node1 nginx]# kubectl get pod -o wide
    NAME                                READY   STATUS    RESTARTS   AGE     IP              NODE    NOMINATED NODE   READINESS GATES
    nginx-deployment-65998d8886-lp45t   1/1     Running   0          5m16s   10.244.104.10   node2   <none>           <none>
    nginx-deployment-65998d8886-wcs9r   1/1     Running   0          5m16s   10.244.104.8    node2   <none>           <none>
    nginx-deployment-65998d8886-xk6wk   1/1     Running   0          5m16s   10.244.104.9    node2   <none>           <none>
    
**注：安全考虑，默认配置下k8s不会将Pod调度到Master节点，若将k8s-master也当作Node使用，则**

    #Master作为Node节点应用
    $ kubectl taint node node1 node-role.kubernetes.io/master -
    
    #恢复Master Only状态
    $ kubectl taint node node1 node-role.kubernetes.io/master="":NoSchedule
    
##### 1-4.Failover 

    ###模拟机器故障，如node2节点故障(关闭node2)
    [root@node1 nginx]# kubectl get node
    NAME    STATUS     ROLES    AGE   VERSION
    node1   Ready      master   42d   v1.13.3
    node2   NotReady   <none>   42d   v1.13.3   ###node2节点被关闭了
    node3   Ready      <none>   42d   v1.13.3
    
    ###查看pod信息,此时pod被调度到node3运行
    [root@node1 nginx]# kubectl get pod -o wide
    NAME                                READY   STATUS        RESTARTS   AGE   IP             NODE    NOMINATED NODE   READINESS GATES
    nginx-deployment-65998d8886-br7cv   1/1     Running       0          65s   10.244.135.7   node3   <none>           <none>
    nginx-deployment-65998d8886-lp45t   0/1     Terminating   0          83m   <none>         node2   <none>           <none>
    nginx-deployment-65998d8886-tdx8l   1/1     Running       0          65s   10.244.135.6   node3   <none>           <none>
    nginx-deployment-65998d8886-w6jfv   1/1     Running       0          65s   10.244.135.5   node3   <none>           <none>
    nginx-deployment-65998d8886-wcs9r   0/1     Terminating   0          83m   <none>         node2   <none>           <none>
    nginx-deployment-65998d8886-xk6wk   0/1     Terminating   0          83m   <none>         node2   <none>           <none>
    
    ###重启node2后，会删除Terminating状态的pod,不过已经运行的pod不会重新调度回node2
    [root@node1 nginx]# kubectl get pod -o wide
    NAME                                READY   STATUS    RESTARTS   AGE    IP             NODE    NOMINATED NODE   READINESS GATES
    nginx-deployment-65998d8886-br7cv   1/1     Running   0          2m7s   10.244.135.7   node3   <none>           <none>
    nginx-deployment-65998d8886-tdx8l   1/1     Running   0          2m7s   10.244.135.6   node3   <none>           <none>
    nginx-deployment-65998d8886-w6jfv   1/1     Running   0          2m7s   10.244.135.5   node3   <none>           <none>
    
    ###删除nginx-deployment
    [root@node1 nginx]# kubectl delete deployment nginx-deployment
    deployment.extensions "nginx-deployment" deleted
    
##### 1-5.用label控制pod的位置

    Scheduler会将Pod调度到所有可用的Node.
    
    但有时希望将Pod部署到指定Node,如：将大量磁盘I/O的Pod部署到配置了SSD的Node 或 Pod需要GPU,需要运行在配置了GPU的节点上
    
    以上：k8s通过label实现该功能
    
    -----------
    
    label是key-value对，各种资源均可设置label,灵活添加各种自定义属性
    
    ###标注node3是配置了SSD的节点
    [root@node1 nginx]# kubectl label node node3 disktype=ssd
    node/node3 labeled
    
    ###查看节点的label
    [root@node1 nginx]# kubectl get node --show-labels
    NAME    STATUS   ROLES    AGE   VERSION   LABELS
    node1   Ready    master   42d   v1.13.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node1,node-role.kubernetes.io/master=
    node2   Ready    <none>   42d   v1.13.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node2
    node3   Ready    <none>   42d   v1.13.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disktype=ssd,kubernetes.io/hostname=node3  ###自定义disktype=ssd的label
    
    ###设置disktype这个自定义label，即可指定将pod部署到node3
    ###编辑nginx.yaml
    [root@node1 nginx]# cat nginx.yaml
    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
      name: nginx-deployment
    spec:
      replicas: 3
      template:
        metadata:
          labels:
            app: web_server
        spec:
          containers:
          - name: nginx
            image: nginx:1.7.9
          nodeSelector:     ###增加，指定将此pod部署到具有label disktype=ssd的node上
            disktype: ssd
            
    ###部署deployment并查看pod运行节点(全部部署到了node3)
    [root@node1 nginx]# kubectl apply -f nginx.yaml 
    deployment.extensions/nginx-deployment created
    [root@node1 nginx]# kubectl get deployment
    NAME               READY   UP-TO-DATE   AVAILABLE   AGE
    nginx-deployment   3/3     3            3           9s

    [root@node1 nginx]# kubectl get pod -o wide
    NAME                                READY   STATUS    RESTARTS   AGE   IP              NODE    NOMINATED NODE   READINESS GATES
    nginx-deployment-7c75d8cdf6-mftr2   1/1     Running   0          19s   10.244.135.8    node3   <none>           <none>
    nginx-deployment-7c75d8cdf6-td57j   1/1     Running   0          19s   10.244.135.10   node3   <none>           <none>
    nginx-deployment-7c75d8cdf6-vbxnq   1/1     Running   0          19s   10.244.135.9    node3   <none>           <none>
    
    
    ###删除label disktype
    [root@node1 nginx]# kubectl label node node3 disktype-
    node/node3 labeled
    [root@node1 nginx]# kubectl get node --show-labels
    NAME    STATUS   ROLES    AGE   VERSION   LABELS
    node1   Ready    master   42d   v1.13.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node1,node-role.kubernetes.io/master=
    node2   Ready    <none>   42d   v1.13.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node2
    node3   Ready    <none>   42d   v1.13.3   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node3  ###自定义disktype=ssd已经被删除
    [root@node1 nginx]# kubectl get pod -o wide
    NAME                                READY   STATUS    RESTARTS   AGE   IP              NODE    NOMINATED NODE   READINESS GATES
    nginx-deployment-7c75d8cdf6-mftr2   1/1     Running   0          99s   10.244.135.8    node3   <none>           <none>
    nginx-deployment-7c75d8cdf6-td57j   1/1     Running   0          99s   10.244.135.10   node3   <none>           <none>
    nginx-deployment-7c75d8cdf6-vbxnq   1/1     Running   0          99s   10.244.135.9    node3   <none>           <none>
    
    注：
    
    上述删除label disktype,pod并不会重新部署，依然在node3节点上运行
    
    除非nginx.yaml中删除nodeSelector设置，然后通过kubectl apply重新部署。k8s会删除之前的pod并调度和运行新pod
    
    ###删除nodeSelector,重新部署
    [root@node1 nginx]# kubectl apply -f nginx.yaml 
    deployment.extensions/nginx-deployment configured
    [root@node1 nginx]# kubectl get pod -o wide
    NAME                                READY   STATUS    RESTARTS   AGE   IP              NODE    NOMINATED NODE   READINESS GATES
    nginx-deployment-65998d8886-h5482   1/1     Running   0          17s   10.244.104.11   node2   <none>           <none>
    nginx-deployment-65998d8886-jz5rq   1/1     Running   0          17s   10.244.135.11   node3   <none>           <none>
    nginx-deployment-65998d8886-l9blg   1/1     Running   0          16s   10.244.104.12   node2   <none>           <none>

#### 2。DaemonSet    

    Deployment部署的副本Pod会分布在各个Node上，每个Node都可能运行好几个副本

    DaemonSet不同处在于：每个Node上最多只能运行一个副本
    
>DaemonSet典型应用场景

    (1).在集群的每个节点上运行存储Daemon,如: glusterd / ceph
    (2).在每个节点上运行日志收集Daemon,如: flunentd / logstash
    (3).在每个节点上运行监控Daemon,如: prometheus Node Exporter / collectd
    
>k8s应用DaemonSet运行系统组件

    ###若不指定namespace,则默认namespace为default
    [root@node1 nginx]# kubectl get daemonset --namespace=kube-system
    NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                 AGE
    calico-node   3         3         3       3            3           beta.kubernetes.io/os=linux   42d
    kube-proxy    3         3         3       3            3           <none>                        42d
    
    ###上述calico-node和kube-proxy分别负责在每个节点上运行calico和kube-proxy组件
    
    ### calico网络，也可以是flannel网络(常用)
    
    ### kube-proxy组件，用于将service代理映射到pod(外界访问->service->kube-proxy->pod)
    
##### 2-1。kube-flannel-ds

>部署flannel网络

    ###部署flannel网络
    [root@node1 k8s]# kubectl apply -f  https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
    podsecuritypolicy.extensions/psp.flannel.unprivileged created
    clusterrole.rbac.authorization.k8s.io/flannel created
    clusterrolebinding.rbac.authorization.k8s.io/flannel created
    serviceaccount/flannel created
    configmap/kube-flannel-cfg created
    daemonset.extensions/kube-flannel-ds-amd64 created
    daemonset.extensions/kube-flannel-ds-arm64 created
    daemonset.extensions/kube-flannel-ds-arm created
    daemonset.extensions/kube-flannel-ds-ppc64le created
    daemonset.extensions/kube-flannel-ds-s390x created
    
    ###查看k8s应用DaemonSet运行的系统组件
    [root@node1 k8s]# kubectl get daemonset --namespace=kube-system
    NAME                      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                     AGE
    calico-node               3         3         3       3            3           beta.kubernetes.io/os=linux       42d
    kube-flannel-ds-amd64     3         3         3       3            3           beta.kubernetes.io/arch=amd64     4m26s
    kube-flannel-ds-arm       0         0         0       0            0           beta.kubernetes.io/arch=arm       4m26s
    kube-flannel-ds-arm64     0         0         0       0            0           beta.kubernetes.io/arch=arm64     4m26s
    kube-flannel-ds-ppc64le   0         0         0       0            0           beta.kubernetes.io/arch=ppc64le   4m26s
    kube-flannel-ds-s390x     0         0         0       0            0           beta.kubernetes.io/arch=s390x     4m26s
    kube-proxy                3         3         3       3            3           <none>                            42d
    
    ###以上：flannel的DaemonSet即定义在kube-flannel.yaml中
    $ wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
    
[kube-flannel.yaml](../resources/kube-flannel.yml)

    ###截取一小部分说明。DaemonSet的语法结构同Deployment。如：
    ---
    apiVersion: extensions/v1beta1
    kind: DaemonSet  ###指定kind为DaemonSet
    metadata:
      name: kube-flannel-ds-amd64
      namespace: kube-system
      labels:
        tier: node
        app: flannel
    spec:
      template:
        metadata:
          labels:
            tier: node
            app: flannel
        spec:
          hostNetwork: true  ###指定hostNetwork,表指定Pod直接使用的是Node网络，相当于docker run --network=host
          nodeSelector:
            beta.kubernetes.io/arch: amd64
          tolerations:
          - operator: Exists
            effect: NoSchedule
          serviceAccountName: flannel
          initContainers:
          - name: install-cni
            image: quay.io/coreos/flannel:v0.11.0-amd64
            command:
            - cp
            args:
            - -f
            - /etc/kube-flannel/cni-conf.json
            - /etc/cni/net.d/10-flannel.conflist
            volumeMounts:
            - name: cni
              mountPath: /etc/cni/net.d
            - name: flannel-cfg
              mountPath: /etc/kube-flannel/
          containers:    ###定义了运行flannel服务的容器
          - name: kube-flannel
            image: quay.io/coreos/flannel:v0.11.0-amd64
    
##### 2-2。kube-proxy

    ###由于无法拿到kube-proxy的YAML文件，只能运行下述命令查看配置：
    $ kubectl edit daemonset kube-proxy --namespace=kube-system

[kube-proxy.yaml](../resources/kube-proxy.yaml)

##### 2-3。运行自己的DaemonSet

 >以Prometheus Node Exporter为例演示用户如何运行自己的DaemonSet
 
    Prometheus是流行的系统监控方案
    
    Node Exporter是Prometheus的agent,以Daemon的形式运行在每个被监控节点上
    
    ###方式一：直接在Docker中运行Node Exporter容器
    [root@node1 k8s]# docker run -d \
    > -v "/proc:/host/proc" \
    > -v "/sys:/host/sys" \
    > -v "/:/rootfs" \
    > --net=host \
    > prom/node-exporter \
    > --path.procfs /host/proc \
    > --path.sysfs /host/sys \
    > --collector.filesystem.ignored-mount-points "^/(sys|proc|dev|host|etc)($|/)"
    
    ###方式二：转换为DaemonSet的YAML配置文件node_exporter.yml
    
[node_exporter.yaml](../resources/node_exporter.yml)   

    ###部署应用
    [root@node1 k8s]# kubectl apply -f node_exporter.yml 
    daemonset.extensions/node-exporter-daemonset created
    
    ###查看Pod信息,node2和node3上分别运行了1个pod
    [root@node1 k8s]# kubectl get pod -o wide | grep node-exporter
    node-exporter-daemonset-2xhzq       1/1     Running   0          3m45s   192.168.1.33    node3   <none>           <none>
    node-exporter-daemonset-h299s       1/1     Running   0          3m45s   192.168.1.32    node2   <none>           <none>

#### 3。Job

容器按运行时间分为两类：

>服务类容器:持续提供服务，需一直运行(如:Http Server/Daemon等)

    k8s的Deployment,ReplicaSet,DaemonSet都用于管理服务类容器

>工作类容器:一次性任务(如：批处理程序，完成后容器就退出)

    Job管理工作类容器
    
创建简是单的Job配置文件[myjob.yml](../resources/myjob.yml)

    [root@node1 k8s]# cat myjob.yml 
    apiVersion: batch/v1  #当前Job的apiVersion
    kind: Job #指明当前资源的类型为Job
    metadata:
      name: myjob
    spec:
      template:
        metadata:
          name: myjob
        spec:
          containers:
          - name: hello
            image: busybox
            command: ["echo","hello k8s Job!"]
          restartPolicy: Never #指定什么情况需重启容器，对于Job只能设置Never/OnFailure.对于其他Controller(如Deployment),可设置为Always

    ###启动Job
    [root@node1 k8s]# kubectl apply -f myjob.yml 
    job.batch/myjob created
    
    ###查看Job状态(按预期启动了一个Pod，并已成功执行)
    [root@node1 k8s]# kubectl get job
    NAME    COMPLETIONS   DURATION   AGE
    myjob   1/1           14s        6m20s
    
    ###查看Pod状态
    [root@node1 k8s]# kubectl get pod
    NAME                                READY   STATUS      RESTARTS   AGE
    myjob-k7sfw                         0/1     Completed   0          7m27s
    
    ###查看Pod的标准输出
    [root@node1 k8s]# kubectl logs myjob-k7sfw
    hello k8s Job!
    
##### 3-1.Pod失败情况

>验证1：myjob.yml/restartPolicy为Never

    ###删除job
    [root@node1 k8s]# kubectl delete -f myjob.yml 
    job.batch "myjob" deleted
   
    ###模拟创建Pod失败，修改myjob.yml中
    command: ["invalide command","hello k8s Job!"]  ##使其异常
    
    ###运行新的Job并查看状态
    [root@node1 k8s]# kubectl get job
    NAME    COMPLETIONS   DURATION   AGE
    myjob   0/1           13s        13s
    
    ###此时，COMPLETIONS的Pod数量为0，查看Pod状态,状态均不正常
    [root@node1 k8s]# kubectl get pod -o wide
    NAME                                READY   STATUS               RESTARTS   AGE    IP              NODE    NOMINATED NODE   READINESS GATES
    myjob-g877f                         0/1     ContainerCreating    0          13s    <none>          node3   <none>           <none>
    myjob-gxk4v                         0/1     ContainerCannotRun   0          29s    10.244.135.13   node3   <none>           <none>

    ###查看某个pod的启动日志
    $ kubectl describe pod myjob-gxk4v
    
kubectl get pod -o wide会查询多个失败Pod?

    当第一个Pod启动时，容器失败退出。
    
    根据restartPolicy: Never，此时失败容器不会被重启。但Job DESIRED的Pod是1.当前COMPLETIONS为0不满足。所以Job Controller会启动新的Pod，直到COMPLETIONS为1
    
若设置restartPolicy: OnFailure?

    [root@node1 k8s]# kubectl apply -f myjob.yml 
    job.batch/myjob created
    
    [root@node1 k8s]# kubectl get job
    NAME    COMPLETIONS   DURATION   AGE
    myjob   0/1           4s         4s      ###COMPLETIONS数量为0
    
    [root@node1 k8s]# kubectl get pod 
    NAME                                READY   STATUS              RESTARTS   AGE
    myjob-ltnrm                         0/1     RunContainerError   2          75s  ##此时RESTARTS数量为2
    
    ###上述说明：OnFailure配置生效，容器失败后会重启，重启次数为2
    
    ###删除job
    [root@node1 k8s]# kubectl delete -f myjob.yml 
    job.batch "myjob" deleted

##### 3-2.Job的并行性

>希望同时运行多个Pod，提高Job的执行效率 

    [root@node1 k8s]# cat myjob.yml 
    apiVersion: batch/v1
    kind: Job
    metadata:
      name: myjob
    spec:
      completions: 2  #每次并行执行2个pod,直到总数有2个(可任意设置)pod成功完成.默认1
      parallelism: 2  #每次并行执行2个Pod,默认1
      template:
        metadata:
          name: myjob
        spec:
          containers:
          - name: hello
            image: busybox
            command: ["echo","hello k8s Job!"]
          restartPolicy: OnFailure

    ###查看job
    [root@node1 k8s]# kubectl get job
    NAME    COMPLETIONS   DURATION   AGE
    myjob   2/2           16s        17s
    
    ###查看pod,AGE一样，说明是并行执行
    [root@node1 k8s]# kubectl get pod
    NAME                                READY   STATUS      RESTARTS   AGE
    myjob-t8sfj                         0/1     Completed   0          21s
    myjob-zlfq2                         0/1     Completed   0          21s
    
    ###需要并行处理场景：批处理程序，每个副本(Pod)都会从任务池中读取任务并执行，副本越多，执行时间越短，效率越高(均可用job执行)

##### 3-3.定时Job
    
>linux中有cron程序定时执行任务，k8s的CronJob提供了类似功能，可定时执行Job

    #定时Job
    [root@node1 k8s]# cat mycronjob.yml 
    apiVersion: batch/v2alpha1  #batch/v2alpha1当前CronJob的apiVersion
    kind: CronJob #指明当前资源类型为CronJob
    metadata:
      name: hello
    spec:
      schedule: "*/1 * * * *" #指定什么时候运行Job,格式与linux cron一致,每分钟启动一次
      jobTemplate: #定义job的模版
        spec:
          template:
            spec:
              containers:
              - name: hello
                image: busybox
                command: ["echo","hello k8s CronJob!"]
              restartPolicy: OnFailure
    
    
    ###创建CronJob
    [root@node1 k8s]# kubectl apply -f mycronjob.yml 
    error: unable to recognize "mycronjob.yml": no matches for kind "CronJob" in version "batch/v2alpha1"
    
    ###此时报错：因为k8s默认没有enable CronJob功能，需在kube-apiserver中加入这个功能
    [root@node1 k8s]# vim /etc/kubernetes/manifests/kube-apiserver.yaml 
    - kube-apiserver
    - --runtime-config=batch/v2alpha1=true #add此行
    
    ###重启kubelet服务,此时kubelet会重启kube-apiservice Pod
    [root@node1 k8s]# systemctl restart kubelet.service
    
    ###通过kubectl api-versions确认kube-apiserver现已支持batch/v2alpha1
    [root@node1 k8s]# kubectl api-versions
    admissionregistration.k8s.io/v1beta1
    apiextensions.k8s.io/v1beta1
    apiregistration.k8s.io/v1
    apiregistration.k8s.io/v1beta1
    apps/v1
    apps/v1beta1
    apps/v1beta2
    authentication.k8s.io/v1
    authentication.k8s.io/v1beta1
    authorization.k8s.io/v1
    authorization.k8s.io/v1beta1
    autoscaling/v1
    autoscaling/v2beta1
    autoscaling/v2beta2
    batch/v1
    batch/v1beta1
    batch/v2alpha1   ###支持
    certificates.k8s.io/v1beta1
    coordination.k8s.io/v1beta1
    crd.projectcalico.org/v1
    events.k8s.io/v1beta1
    extensions/v1beta1
    networking.k8s.io/v1
    policy/v1beta1
    rbac.authorization.k8s.io/v1
    rbac.authorization.k8s.io/v1beta1
    scheduling.k8s.io/v1beta1
    storage.k8s.io/v1
    storage.k8s.io/v1beta1
    v1
    
    ###再次创建CronJob
    [root@node1 k8s]# kubectl apply -f mycronjob.yml 
    cronjob.batch/hello created
    
    ###查看cronjob状态
    [root@node1 k8s]# kubectl get cronjob
    NAME    SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
    hello   */1 * * * *   False     0        <none>          12s
    
    ###查看job执行情况,每分钟会启动一个Job
    [root@node1 k8s]# kubectl get jobs
    NAME               COMPLETIONS   DURATION   AGE
    hello-1557481620   1/1           10s        2m28s
    hello-1557481680   1/1           10s        88s
    hello-1557481740   1/1           10s        28s
    
    ###查看pod
    [root@node1 k8s]# kubectl get pod
    NAME                                READY   STATUS              RESTARTS   AGE
    hello-1557481740-268mk              0/1     Completed           0          3m4s
    hello-1557481800-g9drk              0/1     Completed           0          2m4s
    
    ###查看pod运行日志
    [root@node1 k8s]# kubectl logs hello-1557481800-g9drk
    hello k8s CronJob!
    
    ###删除cronjob
    [root@node1 k8s]# kubectl delete -f mycronjob.yml 
    cronjob.batch "hello" deleted
    
--------------------------------------------

**体胖还需勤跑步，人丑就要多读书!!! --开心玉凤**